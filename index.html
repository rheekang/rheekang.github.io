<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" type="text/css" href="styles.css">
    <title>Optimizing DeepGLEAM Model for Flu Prediction</title>
</head>



<body>

    <header class="page-header" role="banner">
        <h1 class="project-name">Optimizing DeepGLEAM Model for Flu Prediction</h1>
        <!-- <h2 class="project-tagline"></h2> -->
        
  <!--      <a href="https://github.com/rheekang/DSC180B-Quarter-2-Project" class="btn">View on GitHub</a> -->
    </header>

    <h2>Introduction</h2>

    <p> The current COVID-19 pandemic and common flu highlight the importance of time- sensitive information in biomedical institutions, 
	    politics, and economics. The application of data science in creating real-time predictive models is crucial to help researchers 
	    and world leaders better understand disease spread and take preventative measures. Our project aims to optimize the DeepGLEAM model, 
	    a deep learning and stochastic process- based predictive model, by better processing raw data and simulating it for improved flu case 
	    predictions in the United States. We close the gap in missing data in the DeepGLEAM model by researching interpolation and imputation 
	    techniques to enhance the model’s predictions. The performance of our adjusted model will be compared against 
	    a control model for effectiveness.
 </p>

    <p> Due to the current state of affairs, the importance of time-sensitive information, especially regarding the livelihood and health of countries 
	    across the world has come to the forefront of biomedical institutions, as well as in politics and economics. Data science, in its essence, 
	    derives necessary analysis and insights amalgamated from computational programming, as well as a fundamental understanding of statistics 
	    and decision-making on a technical level. Consequently, data science is crucial in its application in a vast array of fields and issues, 
	    including the current COVID-19 endemic and the common flu, being able to create real-time predictive models that would enable researchers 
	    and world lead- ers to be more aware of the spread and preemptive measures against disease. Therefore, within our project proposal for Winter 
	    Quarter 2023, we aim to fully grasp the mechan- ics of Professor Yian-Ma and Professor Yu’s DeepGLEAM model and further optimize its ability 
	    to merge simulated and ground truth data so it better predicts future cases of flu cases within the United States. </p>

    <p> Deep learning and stochastic processes can be very effective in better understanding and forecasting future events relevant to our society, 
	    one of which is the recent endemic COVID-19, as well as the common flu. The concept of predictive distributions through previous truths 
	    and predicting with a relative uncertainty projected into the future is quite compelling, for it applies spatiotemporal insights through 
	    quantifiable evidence within code, such as residuals. Because disease is constantly changing, mutating, and affect- ing across state fronts, 
	    weekly data predictions are not only necessary but cumulative in providing cognizance in how illness can permeate and any patterns we as 
	    data scientists can derive from the perpetual change. Our task mainly lies in how we are able to better process the provided raw data and 
	    simulate it so that the forecasting of cases align more closely with future predictions.
    </p>
	<p> The issue with missing data is more inaccurate predictions and its proclivity to increasing uncertainty in predictions. 
		In order to provide our personal contribution towards the already functional DeepGLEAM model, we as a team have researched 
		how to possibly 1) interpolate missing data through a feed-forward system, 2) understand the weight vec- tors of hyperparameters 
		when splitting the model, and 3) considering other possibilities of imputing the missing data, simply due to most states lacking at 
		least a couple weeks of information from the original shape provided within the flu data file. We aim to take the baseline replication of 
		the DeepGLEAM model from Quarter 1, train individual models per each state’s data without tampering with any of it, altering the data 
		through interpolation and imputation, and finally juxtapose the results of the adjusted model’s ability to predict future cases against 
		the control model. </p>

    <h2>Methods and Experiment Design</h2>

	<p> tba </p>

    <h2>What is our Data?</h2>
    <p> tba
    </p>
    

    <h2>Procedure</h2>

    <h3>EDA</h3>

    <p>
        tba </p>

    <h3>Data Cleaning</h3>
    <p>
        tba  </p>

    <h3>Model Fitting and Trajectories</h3>
    <p>
        tba
    </p>
    <h2>Results</h2>
    <p>
        Work in Progress
    </p>

    <h2>Conclusion</h2>
    <p>
        Work in Progress
    </p>

</body>

</html>
